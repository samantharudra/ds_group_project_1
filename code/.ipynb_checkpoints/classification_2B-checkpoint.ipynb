{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0c965bd-acd3-4b2f-900a-c448308ee020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, recall_score\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import fbeta_score, make_scorer, classification_report, confusion_matrix, recall_score\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3fbd4d5-f3b6-478c-9d26-e69ed9172944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in preprocessed data\n",
    "df = pd.read_csv('../data/preprocessed/malware_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b653ab1-1dba-4b9f-abce-955af5b6bfab",
   "metadata": {},
   "source": [
    "#### Model Selection "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227dd89d-d8aa-49a9-b3bc-abe93ba236a3",
   "metadata": {},
   "source": [
    "The goal of this classification task is predicting instances of malware. The dataset is imbalanced, meaning there are more positive cases (malware) compared to negative cases. This imbalance can cause challenges with traditional accuracy metrics, which tend to favor the majority class. We aim to prioritize the minimization of false negatives while also preserving as many benign apps as possible. We will use the 3 models below: \n",
    "\n",
    "1. CART: CART is a decision tree that uses the Gini index as criterion for splitting. It builds binary trees, meaning each node is split into two child nodes. It includes mechanisms for handling missing values and provides built-in pruning methods to avoid overfitting. It can handle non-linear relationships and interactions between features, making it suitable for capturing complex patterns. It is fairly easy to interpret and visualize.\n",
    "\n",
    "2. C5.0: This model is an optimized version of C4.5, with fast execution times and improved memory usage. It uses GR to decide on splits. It has advanced boosting techniques and can handle imbalanced datasets.\n",
    "\n",
    "3. KNN: KNN is a simple yet effective model for classification. It works by classifying an instance based on the majority class of its neighbors. For imbalanced datasets, KNN can perform well if appropriate distance metrics and weighting schemes are applied. KNN is a simple and effective algorithm that does not require training time and can capture complex decision boundaries. While it lacks hyperparameters, it provides a useful benchmark. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e32d593-cb99-4a01-a49a-6fa49c9bcd63",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bf4c71-475c-4e9c-a5c2-be9ca1008ee8",
   "metadata": {},
   "source": [
    "The goal of our algorithm is to capture as many malware cases (minimizing false negatives), while preserving as many benign apps as possible. False negatives (missed instances of malware) are more harmful than false positives (falsely flagging benign apps as malware), however, given the imbalance of the dataset we want to prioritize both precision and recall. Thus, we will use F score as our scoring metric. The F score is a weighted balance of precision and recall. We will use a $\\beta > 1$ since we want to prioritize recall. This will allow us to ensure that the model is capturing as many instances of malware as possible without incorrectly flagging benign apps.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5726725c-e2d0-4443-8c6e-7595ed578770",
   "metadata": {},
   "outputs": [],
   "source": [
    "f2_scorer = make_scorer(fbeta_score, beta=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3573ff5-e040-4021-a253-c1a9928ad3be",
   "metadata": {},
   "source": [
    "#### Split data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be47d332-6ea9-4dc3-8fc4-029e11d97d56",
   "metadata": {},
   "source": [
    "Given that the dataset is imbalanced (with fewer benign samples than malicious ones), stratification prevents the training set from being skewed toward the majority class. Without stratification, the model might underrepresent the minority class (benign apps) in the training set, leading to poor performance in preserving benign apps.\n",
    "\n",
    "The training set will have a balanced representation of both classes and the validation set will accurately reflect the overall class distribution, allowing for better hyperparameter tuning. The test set will also reflect the real-world distribution, making performance evaluation more realistic. Since our goal is to maximize recall (minimize false negatives), maintaining class balance during training and validation is critical.\n",
    "\n",
    "We will also implement K-Fold cross validation since we have a relatively small dataset (n=4464), with stratification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4a3ca61-9e4e-42ed-9018-8829dc6ed827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "X = df.drop('Label', axis=1)\n",
    "y = df['Label']\n",
    "\n",
    "# split using stratification \n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=7)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db30164-cadf-436c-a263-25da26761ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement K-Fold cross-validation since we have a relatively small dataset\n",
    "skf = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a8ea5b-9152-4378-bcfe-a975459c96e0",
   "metadata": {},
   "source": [
    "#### Model 1: CART (Decision Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fde44a-b148-4574-b1d0-203133b2fe2f",
   "metadata": {},
   "source": [
    "CART is a simple, interpretable algorithm that can handle imbalanced data. It splits data based on the Gini index, making it effective for classification tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e686bf0a-81eb-4986-ab2a-cb49495025c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cart_model = DecisionTreeClassifier()\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5], \n",
    "    'class_weight': ['balanced', None]\n",
    "}\n",
    "grid_search_cart = GridSearchCV(cart_model, param_grid, scoring=f2_scorer, cv=skf)\n",
    "grid_search_cart.fit(X_val, y_val)\n",
    "\n",
    "# Use the validation set for hyperparameter tuning\n",
    "best_cart_model = grid_search_cart.best_estimator_\n",
    "\n",
    "# Evaluate the best model using cross-validation on the training data\n",
    "cross_val_scores = cross_val_score(best_cart_model, X_train, y_train, cv=skf, scoring=f2_scorer)\n",
    "print(f\"Cross-validation ROC AUC scores (TRAINING): {cross_val_scores}\")\n",
    "print(f\"Mean ROC AUC (TRAINING): {cross_val_scores.mean():.3f}\")\n",
    "\n",
    "# Evaluate CART\n",
    "\n",
    "best_cart_model.fit(X_train, y_train)\n",
    "\n",
    "cart_preds = best_cart_model.predict(X_test)\n",
    "print(\"CART Classification Report:\\n\", classification_report(y_test, cart_preds))\n",
    "print(\"CART Confusion Matrix:\\n\", confusion_matrix(y_test, cart_preds))\n",
    "\n",
    "# Calculate and print recall\n",
    "cart_recall = recall_score(y_test, cart_preds)\n",
    "print(f\"CART Recall: {cart_recall:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7c7b1c-c82f-4ddb-8e8c-2631376a75d3",
   "metadata": {},
   "source": [
    "#### Model 2: C5.0 (XGBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d1a9cb-9236-47d4-997b-35e4a2f7fd21",
   "metadata": {},
   "source": [
    "C5.0 (via XGBoost) is known for high accuracy and can naturally handle imbalance. It also supports regularization and boosting, reducing overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dee717ba-756f-4d32-9edb-1f9cc1331769",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', scale_pos_weight=len(y_train[y_train==0])/len(y_train[y_train==1]))\n",
    "param_grid_xgb = {\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'n_estimators': [100, 200, 300]\n",
    "}\n",
    "grid_search_xgb = GridSearchCV(xgb_model, param_grid_xgb, scoring=f2_scorer, cv=skf)\n",
    "grid_search_xgb.fit(X_val, y_val)\n",
    "\n",
    "# Use the validation set for hyperparameter tuning\n",
    "best_xgb_model = grid_search_xgb.best_estimator_\n",
    "\n",
    "# Evaluate the best model using cross-validation on the training data\n",
    "cross_val_scores = cross_val_score(best_cart_model, X_train, y_train, cv=skf, scoring=f2_scorer)\n",
    "print(f\"Cross-validation ROC AUC scores (TRAINING): {cross_val_scores}\")\n",
    "print(f\"Mean ROC AUC (TRAINING): {cross_val_scores.mean():.3f}\")\n",
    "\n",
    "# Evaluate XGBoost\n",
    "\n",
    "best_xgb_model.fit(X_train, y_train)\n",
    "\n",
    "xgb_preds = best_xgb_model.predict(X_test)\n",
    "print(\"XGBoost Classification Report:\\n\", classification_report(y_test, xgb_preds))\n",
    "print(\"XGBoost Confusion Matrix:\\n\", confusion_matrix(y_test, xgb_preds))\n",
    "\n",
    "# Calculate and print recall\n",
    "xgb_recall = recall_score(y_test, xgb_preds)\n",
    "print(f\"XGBoost Recall: {xgb_recall:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84604124-6688-4278-857b-8d220242cf13",
   "metadata": {},
   "source": [
    "#### Model 3: K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a7d5c5-f457-4128-a5d3-ff582c09a174",
   "metadata": {},
   "source": [
    "KNN is a simple and effective algorithm that does not require training time and can capture complex decision boundaries. While it lacks hyperparameters, it provides a useful benchmark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ca100e6-4aa4-4e83-9a0c-c4d4e321e241",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate KNN\n",
    "knn_preds = knn_model.predict(X_test)\n",
    "print(\"KNN Classification Report:\\n\", classification_report(y_test, knn_preds))\n",
    "print(\"KNN Confusion Matrix:\\n\", confusion_matrix(y_test, knn_preds))\n",
    "\n",
    "# Calculate and print recall\n",
    "knn_recall = recall_score(y_test, knn_preds)\n",
    "print(f\"KNN Recall: {knn_recall:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3360ec-923a-4a65-9115-905f28fc9ef6",
   "metadata": {},
   "source": [
    "#### Model Interpretability (SHAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b6fd75-6824-46c7-922e-25077e54fb2d",
   "metadata": {},
   "source": [
    "I used SHAP (SHapley Additive exPlanations) for model interpretability because it provides clear and consistent explanations for the predictions made by complex machine learning models. \n",
    "\n",
    "SHAP calculates the contribution of each feature to the prediction (positive or negative). If a feature significantly increases the likelihood of an app being classified as malicious, it will have a strong positive SHAP value.\n",
    "\n",
    "Plots: \n",
    "- Force Plot: This visualizes how much each feature pushes the model's output toward predicting either \"benign\" or \"malicious.\"\n",
    "- Summary Plot: It shows the average impact of each feature across all predictions, helping identify the most influential predictors in the model.\n",
    "\n",
    "This approach ensures that security analysts can understand which app characteristics are most indicative of malware, helping improve threat detection and prevention strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "36416f13-5dff-4bc9-aaf5-4357ec8c7705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain the CART model\n",
    "explainer_cart = shap.Explainer(best_cart_model, X_test)\n",
    "shap_values_cart = explainer_cart(X_test)\n",
    "shap.summary_plot(shap_values_cart, X_test)\n",
    "\n",
    "# Explain the XGBoost model\n",
    "explainer_xgb = shap.Explainer(best_xgb_model, X_test)\n",
    "shap_values_xgb = explainer_xgb(X_test)\n",
    "shap.summary_plot(shap_values_xgb, X_test)\n",
    "\n",
    "# Explain the KNN model\n",
    "explainer_knn = shap.KernelExplainer(knn_model.predict, X_test)\n",
    "shap_values_knn = explainer_knn.shap_values(X_test)\n",
    "shap.summary_plot(shap_values_knn, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc911158-0255-41ad-98ea-558720c18a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the tree using sklearn\n",
    "plt.figure(figsize=(12, 12))\n",
    "plot_tree(best_cart_model, feature_names = Xtrain.columns,\n",
    "          class_names = list(set(ytest)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e172a4cc-baf7-47bf-87ca-4d07618816ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(Xtrain.columns, cart.feature_importances_)\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Feature Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c13711d-e3f2-494f-a21d-15613984af2e",
   "metadata": {},
   "source": [
    "#### Results and Conclusion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8b92c7f-5fc6-4282-a9a9-ff270f728e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare performance - F1\n",
    "cart_f2 = f2_scorer(y_test, best_cart_model.predict_proba(X_test)[:, 1])\n",
    "xgb_f2 = f2_scorer(y_test, best_xgb_model.predict_proba(X_test)[:, 1])\n",
    "knn_f2 = f2_scorer(y_test, knn_model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "print(f\"CART F-METRIC: {cart_f2:.3f}\")\n",
    "print(f\"XGBoost F-METRIC: {xgb_f2:.3f}\")\n",
    "print(f\"KNN F-METRIC: {knn_f2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f6b3b60f-aeee-4184-90c8-1cd607ae67e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "sns.heatmap(confusion_matrix(y_test, cart_preds), annot=True, fmt='d', ax=axes[0])\n",
    "axes[0].set_title('CART Confusion Matrix')\n",
    "sns.heatmap(confusion_matrix(y_test, xgb_preds), annot=True, fmt='d', ax=axes[1])\n",
    "axes[1].set_title('XGBoost Confusion Matrix')\n",
    "sns.heatmap(confusion_matrix(y_test, knn_preds), annot=True, fmt='d', ax=axes[2])\n",
    "axes[2].set_title('KNN Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46218ed5-9f72-488f-a91d-9f9c5ef7c7e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
